{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ID3 Algorithm\n",
    "\n",
    "**ID3 (Iterative Dichotomizer 3)**: goes through every feature to find the most valuable attribute, then splits based on that attribute. Moves down the tree until it either has a pure class or has met a terminating condition\n",
    "\n",
    "\n",
    "**Requirements for ID3:**\n",
    "- outcomes must be binary\n",
    "- attributes used to build the tree must be categorical (and can have many categories)\n",
    "- categories must be known and countable\n",
    "\n",
    "**Entropy formula** This is the definition for Shannon Entropy. \n",
    "$$ H = \\sum_{i=1}^n P(x_i)log_2 \\frac{1}{P(x_i)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Entropy \n",
    "\n",
    "Example: Let's say we have 20 students, and we're trying to classify them as male and female. The only attribute we have is whether their height is tall, medium, or short.\n",
    "\n",
    "12 boys\n",
    "- 4 tall\n",
    "- 6 medium\n",
    "- 2 short\n",
    "\n",
    "8 girls\n",
    "- 1 tall\n",
    "- 2 medium\n",
    "- 5 short\n",
    "\n",
    "What is the entropy, both before any rule is applied and then after applying a rule for being tall?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial entropy is just the formula plugged in over both the possible classes of interest:\n",
    "\n",
    "$$ H = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "\n",
    "$$ = \\frac{12}{20}*log_2 \\frac{20}{12} + \\frac{8}{20}*log_2 \\frac{20}{8} = .971 $$\n",
    "\n",
    "\n",
    "What if we apply the rule _height = short_? We need to calculate the weighted average of the two entropies, one for the short students and one for the non-short students.\n",
    "\n",
    "$$ H(short) = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "$$ = \\frac{2}{7}*log_2 \\frac{7}{2} + \\frac{5}{7}*log_2 \\frac{7}{5} = .863 $$\n",
    "\n",
    "$$ H(not\\_short) = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "$$ = \\frac{10}{13}*log_2 \\frac{13}{10} + \\frac{3}{13}*log_2 \\frac{13}{3} = .779 $$\n",
    "\n",
    "Note that all the probabilities here are conditional on the criteria we're assuming (short or not short). Now the weighted average of the two is just:\n",
    "\n",
    "$$ P(short) * H(short) + P(not\\_short) * H(not\\_short) = \\frac{7}{20} * .863 + \\frac{13}{20} * .779 = .809 $$\n",
    "\n",
    "So our entropy from this question would go from .971 to .809. That's an improvement. Use the space below to calculate the entropy of the other criteria, for we asked whether the students were tall or medium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule: height = medium\n",
      " 0.9245112497836532\n",
      "\n",
      "Rule: height = tall\n",
      " 0.9280757477080679\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "\n",
    "#Calculate entropy for tall and medium\n",
    "## First, medium:\n",
    "\n",
    "H_med = (6/8) * (log2(8/6)) + (2/8) * (log2(8/2))\n",
    "H_not_med = (6/12) * log2(12/6) + (6/12) * (log2(12/6))\n",
    "H_med_w = (8/20) * H_med + (12/20) * H_not_med\n",
    "print('Rule: height = medium\\n',H_med_w)\n",
    "\n",
    "#tall\n",
    "H_tall = (4/5) * (log2(5/4)) + (1/5) * (log2(5/1))\n",
    "H_not_tall = (8/15) * (log2(15/8)) + (7/15) * (log2(15/7))\n",
    "H_tall_w = (5/20) * H_tall + (15/20) * H_not_tall\n",
    "print('\\nRule: height = tall\\n',H_tall_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Which entropy offers the most information gain? Short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocoding the Algorithm\n",
    "\n",
    "**Pseudocode** is the processes of writing the steps and logic you would implement in code, but in normal language rather than commands a programming language could execute. It can be a useful way to chart out how you want to build an algorithm and is a common topic for technical interviews. Here we'll use pseudocode to explain the ID3 algorithm.\n",
    "\n",
    "Here is reasonable pseudocode for ID3, which we'll then follow up with an explanation of the steps. The outcome for this variable will be A or B. An attribute is denoted a<sub>i</sub>. A value of that attribute is v<sub>i</sub>.\n",
    "\n",
    "\n",
    "<pre>\n",
    "Algorithm(Observations, Outcome, Attributes)\n",
    "    Create a root node.\n",
    "    If all observations are 'A', label root node 'A' and return.\n",
    "    If all observations are 'B', label root node 'B' and return.\n",
    "    If no attributes return the root note labeled with the most common Outcome.\n",
    "    Otherwise, start:\n",
    "        For each value v<sub>i</sub> of each attribute a<sub>i</sub>, calculate the entropy.\n",
    "        The attribute a<sub>i</sub> and value v<sub>i</sub> with the lowest entropy is the best rule.\n",
    "        The attribute for this node is then a<sub>i</sub>\n",
    "            Split the tree to below based on the rule a<sub>i</sub> = v<sub>i</sub>\n",
    "            Observations<sub>v<sub>i</sub></sub> is the subset of observations with value v<sub>i</sub>\n",
    "            If Observations<sub>v<sub>i</sub></sub> is empty cap with node labeled with most common Outcome\n",
    "            Else at the new node start a subtree (Observations<sub>v<sub>i</sub></sub>, Target Outcome, Attributes - {a<sub>i</sub>}) and repeat the algorithm\n",
    "</pre>\n",
    "\n",
    "Now let's walk through this pseudocode algorithm in plain English piece by piece.\n",
    "\n",
    "First you create a root node. Simple enough, you have to start with something.\n",
    "\n",
    "The next two lines say that if you're already exclusively one class, just label with that class and you're done. Similarly the following line says that if you don't have any attributes left you're also done, labeling with the most common outcome.\n",
    "\n",
    "Then we get into the _real_ algorithm. First you have to find the best attribute by calculating entropies for all possible values. The attribute value pair with the lowest entropy is then the best attribute and the attribute you give to the node.\n",
    "\n",
    "You use that rule to split the node, creating subsets of observations. There are then two new nodes, each with a subset of the observations corresponding to the rule. If obsevations are null then label with the dominant outcome.\n",
    "\n",
    "Otherwise at each new node start the algorithm again.\n",
    "\n",
    "This is how a decision tree would actually function. Understanding this should give you some insight into how this algorithm works and reveals several attributes of the algorithm. Firstly, the solution is not necessarily optimal. The tree can get stuck in local optima, just like with the gradient descent algorithm. It also has no way to work backwards if it finds itself in an informationless space. You can add criteria to make it stop before the tree has grown to run out of attributes or all leaf nodes are single classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
